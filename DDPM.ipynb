{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import einx\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from diffusion import Diffusion, UnetConfig\n",
    "import lightning\n",
    "from lightning.fabric import Fabric\n",
    "from pytorch_lightning.utilities.model_summary import ModelSummary\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from typing import Any\n",
    "from torch.utils import data\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DEVICE = \"cuda\"\n",
    "torch.set_float32_matmul_precision('medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "training_set = torchvision.datasets.MNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.MNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True,\n",
    "\t\t\t\t\t\t\t\t\t\t\t  drop_last=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\tdrop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDPM sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPMSampler:\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\t\t  generator:torch.Generator,\n",
    "\t\t\t  num_training_steps=1000,\n",
    "\t\t\t  beta_start:float = 0.00085,\n",
    "\t\t\t  beta_end:float = 0.0120,):\n",
    "\t\t\n",
    "\t\t# scaled linear schedule\n",
    "\t\tself.betas = torch.linspace(beta_start**0.5,beta_end**0.5,num_training_steps,dtype=torch.float32)**2\n",
    "\t\tself.alphas = 1.0 - self.betas\n",
    "\n",
    "\t\tself.alpha_cumprod = torch.cumprod(self.alphas,0)\n",
    "\t\tself.one = torch.tensor(1.0)\n",
    "\t\tself.zero = torch.tensor(0.0)\n",
    "\n",
    "\t\tself.generator = generator\n",
    "\t\tself.num_training_steps = num_training_steps\n",
    "\t\t\n",
    "\t\tself.timesteps = torch.arange(num_training_steps).flip(-1)\n",
    "\n",
    "\tdef add_get_noise(self,\n",
    "\t\t\t\t   original_samples:torch.FloatTensor,\n",
    "\t\t\t\t   timesteps:torch.IntTensor):\n",
    "\t\t\n",
    "\t\talpha_cumprod = self.alpha_cumprod.to(device=original_samples.device,dtype=original_samples.dtype)\n",
    "\t\ttime_steps = timesteps.to(original_samples.device)\n",
    "\t\tsqrt_alpha_cumprod = alpha_cumprod[timesteps]**0.5\n",
    "\t\tsqrt_alpha_cumprod = sqrt_alpha_cumprod.flatten()\n",
    "\n",
    "\t\twhile len(sqrt_alpha_cumprod.shape) < len(original_samples.shape):\n",
    "\t\t\tsqrt_alpha_cumprod = sqrt_alpha_cumprod.unsqueeze(-1)\n",
    "\n",
    "\t\tsqrt_one_minus_alpha_cumprod = (1 - alpha_cumprod[timesteps])**0.5\n",
    "\t\tsqrt_one_minus_alpha_cumprod = sqrt_one_minus_alpha_cumprod.flatten()\n",
    "\n",
    "\t\twhile len(sqrt_one_minus_alpha_cumprod.shape) < len(original_samples.shape):\n",
    "\t\t\tsqrt_one_minus_alpha_cumprod = sqrt_one_minus_alpha_cumprod.unsqueeze(-1)\n",
    "\n",
    "\t\tnoise = torch.randn(original_samples.shape,generator=self.generator,\n",
    "\t\t\t\t\t  device=original_samples.device,dtype=original_samples.dtype)\n",
    "\t\n",
    "\t\tnoisy_sample = sqrt_alpha_cumprod * original_samples + sqrt_one_minus_alpha_cumprod * noise\n",
    "\t\treturn noisy_sample,noise\n",
    "\t\n",
    "\tdef reverse_step(self,\n",
    "\t\t\t\t  timestep:int,\n",
    "\t\t\t\t  latents:torch.Tensor,\n",
    "\t\t\t\t  model_output:torch.Tensor,):\n",
    "\t\t\n",
    "\t\tt = timestep\n",
    "\n",
    "\t\talpha_prod_t = self.alpha_cumprod[timestep]\n",
    "\t\talpha_prod_t_prev = self.alpha_cumprod[timestep]\n",
    "\n",
    "\t\talpha_t = self.alphas[timestep]\n",
    "\n",
    "\t\tsigma_t = ((1 - alpha_prod_t_prev)*(1 - alpha_t)/(1 - alpha_prod_t))**(0.5)\n",
    "\n",
    "\t\timage_scale = 1/(alpha_t**0.5)\n",
    "\n",
    "\t\tnoise_scale = -(1-alpha_t)/(alpha_t*(1-alpha_prod_t))**(0.5)\n",
    "\n",
    "\t\tmean = image_scale*latents + noise_scale*model_output\n",
    "\n",
    "\t\tvariance = 0\n",
    "\t\t\n",
    "\t\tif t> 0:\n",
    "\t\t\tdevice = model_output.device\n",
    "\t\t\tnoise = torch.randn(model_output.shape,\n",
    "\t\t\t\tgenerator=self.generator,\n",
    "\t\t\t\tdevice=device,\n",
    "\t\t\t\tdtype=model_output.dtype)\n",
    "\t\t\tvariance = sigma_t*noise\n",
    "\n",
    "\t\tx_t_prev = mean + variance\n",
    "\n",
    "\t\treturn x_t_prev\n",
    "\t\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IdentityModule(lightning.LightningModule):\n",
    "\n",
    "\tdef __init__(self, *args: Any, **kwargs: Any) -> None:\n",
    "\t\tsuper().__init__(*args, **kwargs)\n",
    "\n",
    "\tdef forward(self,x) -> Any:\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardDiffusion(lightning.LightningModule):\n",
    "\n",
    "\tdef __init__(self,\n",
    "\t\t\t  diffusion_config:UnetConfig,\n",
    "\t\t\t  num_timesteps=1000):\n",
    "\t\t\n",
    "\t\tsuper().__init__()\n",
    "\n",
    "\t\tself.class_embedder = nn.Embedding(11,diffusion_config.d_context)\n",
    "\t\tself.encoder = IdentityModule()\n",
    "\t\tself.decoder = IdentityModule()\n",
    "\t\tself.diffusion = Diffusion(diffusion_config)\n",
    "\t\tself.generator = torch.Generator(device=DEVICE)\n",
    "\t\tself.sampler = DDPMSampler(self.generator,num_training_steps=num_timesteps)\n",
    "\t\tself.diffusion_config = diffusion_config\n",
    "\t\n",
    "\tdef time_embed(self,timesteps):\n",
    "\t\tfreqs = torch.pow(10000,\n",
    "\t\t\t\t\t -torch.arange(start=0,\n",
    "\t\t\t\t\t end=self.diffusion_config.time_dim // 2,\n",
    "\t\t\t\t\t dtype=torch.float32) / (self.diffusion_config.time_dim // 2))\n",
    "\n",
    "\t\t# Expand timesteps to match frequency dimensions (Shape: (batch_size, TIME_DIM // 2))\n",
    "\t\tx = timesteps[:, None] * freqs[None, :].to(timesteps.device)\n",
    "\n",
    "\t\t# Concatenate sine and cosine embeddings (Shape: (batch_size, TIME_DIM))\n",
    "\t\treturn torch.cat([torch.cos(x), torch.sin(x)], dim=-1)\n",
    "\n",
    "\tdef diffusion_training_step(self,x,labels):\n",
    "\t\tb,c,h,w = x.shape\n",
    "\n",
    "\t\tt = torch.randint(0,self.sampler.num_training_steps,\n",
    "\t\t\t\t\t(b,),\n",
    "\t\t\t\t\tdevice=DEVICE,\n",
    "\t\t\t\t\tgenerator=self.generator)\n",
    "\t\t\n",
    "\t\tnoisy_latent,noise = self.sampler.add_get_noise(x,t)\n",
    "\n",
    "\t\ttime_embeds = self.time_embed(t)\n",
    "\n",
    "\t\tcontext = self.class_embedder(labels)\n",
    "\n",
    "\t\tpredicted_noise = self.diffusion(noisy_latent,context,time_embeds)\n",
    "\n",
    "\t\tloss = F.mse_loss(predicted_noise,noise)\n",
    "\t\treturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_normal_diffusion_config = UnetConfig(\n",
    "\tin_channels=1,\n",
    "\tn_downsample=2,\n",
    "\tinit_dim=8,\n",
    "\tfinal_dim=32,\n",
    "\tn_heads=4,\n",
    "\tn_group=4,\n",
    "\tkernel_size=3,\n",
    "\ttime_dim=320,\n",
    "\td_context=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_diff = StandardDiffusion(mnist_normal_diffusion_config,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_diff.sampler.timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_diff.to(DEVICE)\n",
    "ModelSummary(normal_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_prep_batch(x,\n",
    "\t\t\t\t\t\t y,\n",
    "\t\t\t\t\t\t pdrop=0.2):\n",
    "\ty = y+1\n",
    "\tdrop = torch.rand(y.shape)\n",
    "\treturn x,y*(drop>pdrop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diffusion_train(fabric,\n",
    "          model: StandardDiffusion,\n",
    "          training_loader: data.DataLoader,\n",
    "          n_train_steps: int,\n",
    "          val_steps: int):\n",
    "    \n",
    "    n_steps = 0\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    model = model.to(DEVICE)\n",
    "\n",
    "    model.encoder.freeze()\n",
    "    model.decoder.freeze()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=1E-3, weight_decay=1E-3)\n",
    "\n",
    "    model,optimizer = fabric.setup(model,optimizer)\n",
    "    \n",
    "    training_losses = []\n",
    "    \n",
    "    while n_steps < n_train_steps:\n",
    "\n",
    "        for batch in training_loader:\n",
    "            with fabric.autocast():\n",
    "                x, y = diffusion_prep_batch(*batch,pdrop=0.2)\n",
    "                x = x.to(DEVICE)\n",
    "                y = y.to(DEVICE)\n",
    "                \n",
    "                loss = model.diffusion_training_step(x,y)\n",
    "\n",
    "            fabric.backward(loss)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            optimizer.step()\n",
    "            model.zero_grad()\n",
    "            \n",
    "            n_steps += 1\n",
    "\n",
    "            if n_steps % val_steps == 0:\n",
    "                \n",
    "                print(f\"Training step: {n_steps}/{n_train_steps} | Loss: {np.mean(losses)}\")\n",
    "                \n",
    "                training_losses.append(np.mean(losses))\n",
    "\n",
    "                losses = []\n",
    "                \n",
    "            if n_steps >= n_train_steps:\n",
    "                break\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fabric = Fabric(accelerator=\"cuda\",precision=\"32-true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diffusion_train(fabric,normal_diff,training_loader,5000,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = normal_diff.state_dict()\n",
    "torch.save(state_dict,\"states.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_diff.load_state_dict(torch.load(\"states.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_embedding(timestep):\n",
    "    # Shape: (160,)\n",
    "    freqs = torch.pow(10000, -torch.arange(start=0, end=320//2, dtype=torch.float32) / (320//2)) \n",
    "    # Shape: (1, 160)\n",
    "    x = torch.tensor([timestep], dtype=torch.float32)[:, None] * freqs[None]\n",
    "    # Shape: (1, 160 * 2)\n",
    "    return torch.cat([torch.cos(x), torch.sin(x)], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_gen(model:StandardDiffusion,\n",
    "\t\t\t  image_shape,\n",
    "\t\t\t  classes,\n",
    "\t\t\t  do_cfg=True,\n",
    "\t\t\t  cfg_scale=2,):\n",
    "\t\n",
    "\tn_imgs = len(classes)\n",
    "\tlatent_shape = [n_imgs] + list(image_shape)\n",
    "\n",
    "\tlatents = torch.randn(latent_shape,\n",
    "\t\tgenerator=model.generator,\n",
    "\t\tdevice=model.device)\n",
    "\t\n",
    "\ttimesteps = tqdm(model.sampler.timesteps)\n",
    "\n",
    "\tif do_cfg:\n",
    "\t\tuncond_context = torch.zeros_like(classes)\n",
    "\t\tcontext = einx.rearrange(\"b1 ..., b2 ... -> (b1 + b2) ...\",classes,uncond_context)\n",
    "\telse:\n",
    "\t\tcontext = classes\n",
    "\n",
    "\tcontext = model.class_embedder(context)\n",
    "\n",
    "\tfor i,timestep in enumerate(timesteps):\n",
    "\n",
    "\t\twith torch.no_grad():\n",
    "\t\t\ttime_embedding = get_time_embedding(timestep).to(model.device)\n",
    "\n",
    "\t\t\tmodel_inputs = einx.rearrange(\"b ... -> (k b) ...\",latents,k=(1+do_cfg))\n",
    "\n",
    "\t\t\tmodel_output = model.diffusion(model_inputs,context,time_embedding)\n",
    "\n",
    "\t\t\tif do_cfg:\n",
    "\t\t\t\toutput_cond,output_uncond = einx.rearrange(\"(k b) ... -> k b ...\",model_output,k=2)\n",
    "\t\t\t\tmodel_output = cfg_scale * (output_cond - output_uncond) + output_uncond\n",
    "\n",
    "\t\t\tlatents = model.sampler.reverse_step(timestep,latents,model_output)\n",
    "\n",
    "\twith torch.no_grad():\n",
    "\t\timages = model.decoder(latents)\n",
    "\treturn images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = diffusion_prep_batch(*next(iter(validation_loader)),pdrop=0)\n",
    "x = x.to(DEVICE)\n",
    "y = y.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_imgs = mnist_gen(normal_diff,x.shape[1:],\n",
    "\t\t\t\t\t  classes=y,cfg_scale=5,\n",
    "\t\t\t\t\t  do_cfg=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_imgs = diff_imgs.cpu().detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(diff_imgs[i].squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {y[i]-1}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 10, figsize=(15, 5))\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.imshow(x[i].cpu().squeeze(), cmap='gray')\n",
    "    ax.set_title(f'Label: {y[i]-1}', fontsize=12)\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "neuro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
